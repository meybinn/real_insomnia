import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, classification_report

from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from xgboost import XGBClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import roc_auc_score
from imblearn.over_sampling import SMOTE

# ë¬¸ì„œ ê°–ê³ ì˜¤ê¸°
df = pd.read_csv(r'C:\Users\\henry\Desktop\shhs1-dataset-0.20.0.csv', low_memory=False)

# ì£¼ìš” ë³€ìˆ˜ ì„ ì • // ì‚°ì†Œí¬í™”ë„ í‰ê· &ìµœì†Œ, saturation<90/85 íšŸìˆ˜(percentage), ahi
selected_columns = ['avgsat','minsat','pctsa85h','pctsa90h','ahi_a0h3']

df_selected = df[selected_columns].copy()

df_selected.to_csv('shhs_sleep_selected.csv',index=False)

# print("!!!!!!!!!!!!!!!!ì™„ë£Œ!!!!!!!!!!!!!!!!") # ë³€ìˆ˜ ì²˜ë¦¬ê¹Œì§€

# label // ahië¥¼ labelë¡œ ì •ì˜
df_selected['apnea'] = df_selected['ahi_a0h3'].apply(lambda x:1 if(x>=15)else 0)

# print("!!!!!!!!!!!!",df_selected["apnea"].value_counts(),"!!!!!!!!!!!!")

# print("!!!!!!!!!!!!",df_selected["apnea"].value_counts(),"!!!!!!!!!!!!")

# ê²°ì¸¡ì¹˜ ì²˜ë¦¬
  # ê²°ì¸¡ì¹˜ ì²˜ë¦¬ - ê²°ì¸¡ì¹˜ í™•ì¸
# print(df_selected.isna().sum()/len(df_selected))  # ê²°ì¸¡ì¹˜ ë¹„ìœ¨ í™•ì¸ -> ìƒíƒœ ì–‘í˜¸            
# print("null ",df_selected.isnull().sum())  # í™•ì¸ ê²°ê³¼ df_selected[minsat]ì˜ ê²°ì¸¡ì¹˜ëŠ” ëª¨ë‘ nullì„
  # ê²°ì¸¡ì¹˜ ì²˜ë¦¬ - nullê°’ë“¤ ì²˜ë¦¬
df_selected.fillna(df_selected.mode().iloc[0],inplace=True) # ê²°ì¸¡ì¹˜ë¥¼ ê° ì—´ì˜ ìµœë¹ˆê°’ìœ¼ë¡œ ì±„ì›€
# df_selected.fillna(df_selected.median(),inplace=True)  # ê²°ì¸¡ì¹˜ë¥¼ í‰ê· ê°’ìœ¼ë¡œ ì±„ì›€

# ì´ìƒì¹˜ ì²˜ë¦¬  // IQRì ìš©
def remove_outliers_iqr(df,columns):
    for col in columns:
        Q1 = df[col].quantile(0.25) #1ì‚¬ë¶„ìœ„
        Q3 = df[col].quantile(0.75) #3ì‚¬ë¶„ìœ„
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR #ë°•ìŠ¤í”Œë¡¯ã…‰ã…‰
        upper_bound = Q3 + 1.5 * IQR 
        # ì´ìƒì¹˜ ë°”ê¹¥ê°’ì„ NaNìœ¼ë¡œ ì„¤ì • í›„ ìµœë¹ˆê°’ìœ¼ë¡œ ëŒ€ì²´ (ì‚­ì œ ë˜ëŠ” ë‹¤ë¥¸ ë°©ì‹ë„ ê°€ëŠ¥)
        df[col] = df[col].apply(lambda x:np.nan if x < lower_bound or x > upper_bound else x)
        print(f"{col} ì´ìƒì¹˜ ê¸°ì¤€: < {lower_bound:.2f} ë˜ëŠ” > {upper_bound:.2f}")
    return df
  # ì´ìƒì¹˜ ì²˜ë¦¬ - ì´ìƒì¹˜ ì œê±°
df_selected = remove_outliers_iqr(df_selected, ['avgsat','minsat','pctsa85h','pctsa90h','ahi_a0h3']) # pctsa85h ìˆ˜ì¹˜ì¡°ì •
df_selected.fillna(df_selected.mode().iloc[0], inplace=True)

# íŠ¹ì§• í™•ì¥ (íŒŒìƒ ë³€ìˆ˜ ì¶”ê°€)
df_selected['avg_min_ratio'] = df_selected['avgsat'] / (df_selected['minsat'] + 1e-5)
df_selected['pct_diff_90_85'] = df_selected['pctsa90h'] - df_selected['pctsa85h']
df_selected['avg_minus_min'] = df_selected['avgsat'] - df_selected['minsat']


# ë°ì´í„° ë¶„ë¦¬ (x,y,train/test)
X = df_selected.drop(columns=['ahi_a0h3','apnea'])  # íŠ¹ì§• (features, ëª¨ë¸ì´ ì˜ˆì¸¡í•  ì…ë ¥ ë°ì´í„°) - ê°œì¸ ì‹ë³„ì, ë¶ˆë©´ì¦ ì—¬ë¶€, labelì„ ì œê±°í•œ ê°’ë“¤ì´ xë¡œ ì…ë ¥
y = df_selected['apnea'] # ë¼ë²¨ (ì˜ˆì¸¡í•´ì•¼ í•˜ëŠ” ê²°ê³¼ê°’)

# í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë‚˜ëˆ„ê¸°
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,stratify=y, random_state=12)

# ìŠ¤ì¼€ì¼ë§/ì •ê·œí™”  // standardScalar ì‚¬ìš©
# scaled_data = scalar.fit_transform(df_selected)
# print("Standardization ì ìš© í›„ ë°ì´í„°: \n", scaled_data
scalar = StandardScaler()
X_train_scaled = scalar.fit_transform(X_train)
X_test_scaled = scalar.transform(X_test)

# SMOTE ì ìš© (í›ˆë ¨ì…‹ë§Œ)
sm = SMOTE(random_state=42)
X_train_res, y_train_res = sm.fit_resample(X_train_scaled, y_train)

# SVM ëª¨ë¸ ì •ì˜ + GridSearchCVë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
param_grid_svm = {
    'C': [0.1, 1, 10],               # SVMì˜ C íŒŒë¼ë¯¸í„° (ê·œì œ ê°•ë„): ì‘ì„ìˆ˜ë¡ ë§ˆì§„ì´ ë„“ìŒ, í´ìˆ˜ë¡ ì˜¤ì°¨ë¥¼ ì¤„ì´ë ¤ í•¨
    'gamma': ['scale', 0.01, 0.1, 1],# gammaëŠ” RBF ì»¤ë„ì˜ ì˜í–¥ ë²”ìœ„ ì¡°ì ˆ: í´ìˆ˜ë¡ ê°œë³„ ìƒ˜í”Œì˜ ì˜í–¥ì´ í¼
    'kernel': ['rbf'],               # ì»¤ë„ í•¨ìˆ˜ë¡œ RBF(ê°€ìš°ì‹œì•ˆ)ë§Œ ì‚¬ìš© (ë¹„ì„ í˜• ë¶„ë¥˜ì— ê°•í•¨)
    'class_weight': ['balanced']
}

grid_svm = GridSearchCV(SVC(probability=True), param_grid_svm, cv=5, n_jobs=-1)
grid_svm.fit(X_train_res, y_train_res)
print("\n=== SVM ìµœì  íŒŒë¼ë¯¸í„° ===")
print(grid_svm.best_params_)

svm_pred = grid_svm.predict(X_test_scaled)
print("\n=== SVM ê²°ê³¼ ===")
print(confusion_matrix(y_test, svm_pred))
print(classification_report(y_test, svm_pred))

# ğŸ”¥ 3ï¸âƒ£ RandomForest í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
param_grid_rf = {
    'n_estimators': [100, 200],
    'max_depth': [10, 20, None],
    'class_weight': ['balanced']
}
grid_rf = GridSearchCV(RandomForestClassifier(random_state=12), param_grid_rf, cv=5, n_jobs=-1)
grid_rf.fit(X_train_res, y_train_res)
print("\n=== RandomForest ìµœì  íŒŒë¼ë¯¸í„° ===")
print(grid_rf.best_params_)

# ğŸ”¥ 4ï¸âƒ£ XGBoost í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
scale_pos_weight = len(y_train_res[y_train_res == 0]) / len(y_train_res[y_train_res == 1])
param_grid_xgb = {
    'n_estimators': [100, 200],
    'max_depth': [3, 6],
    'learning_rate': [0.05, 0.1],
    'scale_pos_weight': [1, scale_pos_weight]
}
grid_xgb = GridSearchCV(XGBClassifier(eval_metric='logloss', random_state=12), 
                        param_grid_xgb, cv=5, n_jobs=-1)
grid_xgb.fit(X_train_res, y_train_res)
print("\n=== XGBoost ìµœì  íŒŒë¼ë¯¸í„° ===")
print(grid_xgb.best_params_)

# ğŸ”¥ 5ï¸âƒ£ ì•™ìƒë¸” ëª¨ë¸ (Voting)
ensemble = VotingClassifier(
    estimators=[
        ('svm', grid_svm.best_estimator_),
        ('rf', grid_rf.best_estimator_),
        ('xgb', grid_xgb.best_estimator_)
    ],
    voting='soft'
)
ensemble.fit(X_train_res, y_train_res)

# ì˜ˆì¸¡ ë° í‰ê°€ í•¨ìˆ˜
def evaluate_model(name, model, X_test, y_test):
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:, 1]
    print(f"\n=== {name} ê²°ê³¼ ===")
    print(confusion_matrix(y_test, y_pred))
    print(classification_report(y_test, y_pred))
    auc = roc_auc_score(y_test, y_proba)
    print(f"AUC: {auc:.3f}")

# ğŸ”¥ ìµœì¢… í‰ê°€
evaluate_model('SVM', grid_svm.best_estimator_, X_test_scaled, y_test)
evaluate_model('RandomForest', grid_rf.best_estimator_, X_test_scaled, y_test)
evaluate_model('XGBoost', grid_xgb.best_estimator_, X_test_scaled, y_test)
evaluate_model('Ensemble (Voting)', ensemble, X_test_scaled, y_test)

# ì¸ì½”ë”©

# ìƒ˜í”Œë§

# ëª¨ë¸ í•™ìŠµ 